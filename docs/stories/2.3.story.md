# Story 2.3: Mastra Corridor Research Agent with Firecrawl

## Status

**Draft**

---

## Story

**As a** system,
**I want** an AI agent that researches any corridor using multiple data sources,
**so that** users get comprehensive intelligence for any corridor dynamically.

---

## Acceptance Criteria

1. Mastra agent configured with corridor research system prompt
2. Agent tools include: tavilySearch, firecrawlScrape, redditSearch, perplexityQuery
3. Agent accepts origin+destination and returns raw research results
4. Firecrawl configured to scrape: Nairaland, InterNations, Expatica, embassy sites
5. Research results stored in Convex IngestedContent table
6. Agent runs as Convex background action (non-blocking)
7. Agent handles rate limits and retries gracefully

---

## Tasks / Subtasks

- [ ] **Task 1: Install Mastra and Dependencies** (AC: 1)
  - [ ] Install: `npm install @mastra/core @ai-sdk/anthropic`
  - [ ] Create `agents/` directory at project root
  - [ ] Add `ANTHROPIC_API_KEY` to Convex environment
  - [ ] Verify Mastra imports work

- [ ] **Task 2: Create Firecrawl Tool** (AC: 2, 4)
  - [ ] Create `agents/tools/firecrawl.ts`
  - [ ] Install Firecrawl SDK: `npm install @mendable/firecrawl-js`
  - [ ] Implement scrape function for URL → markdown content
  - [ ] Add `FIRECRAWL_API_KEY` to Convex environment
  - [ ] Handle JavaScript-rendered pages

- [ ] **Task 3: Create Tavily Tool** (AC: 2)
  - [ ] Create `agents/tools/tavily.ts`
  - [ ] Install: `npm install @tavily/core`
  - [ ] Implement search function returning URLs + snippets
  - [ ] Add `TAVILY_API_KEY` to Convex environment
  - [ ] Configure for immigration-related queries

- [ ] **Task 4: Create Reddit Tool** (AC: 2)
  - [ ] Create `agents/tools/reddit.ts`
  - [ ] Implement Reddit API search for subreddits
  - [ ] Target: r/IWantOut, r/expats, r/immigration, country-specific
  - [ ] Parse posts and top comments
  - [ ] Add Reddit API credentials to environment

- [ ] **Task 5: Create Perplexity Tool** (AC: 2)
  - [ ] Create `agents/tools/perplexity.ts`
  - [ ] Implement search for real-time policy queries
  - [ ] Add `PERPLEXITY_API_KEY` to Convex environment
  - [ ] Format results with citations

- [ ] **Task 6: Create Corridor Researcher Agent** (AC: 1, 3)
  - [ ] Create `agents/corridorResearcher.ts`
  - [ ] Configure with Claude Sonnet 4 model
  - [ ] Write system prompt for migration research
  - [ ] Register all tools with agent
  - [ ] Define input/output schemas

- [ ] **Task 7: Create Convex Research Action** (AC: 5, 6)
  - [ ] Create `convex/ai/research.ts`
  - [ ] Implement `researchCorridor` action
  - [ ] Execute Mastra agent with corridor context
  - [ ] Store scraped content in IngestedContent table
  - [ ] Generate embeddings for stored content
  - [ ] Run as background action (non-blocking)

- [ ] **Task 8: Implement Rate Limiting and Retries** (AC: 7)
  - [ ] Add Upstash rate limiting per API
  - [ ] Implement exponential backoff for retries
  - [ ] Log rate limit hits for monitoring
  - [ ] Return partial results if some tools fail

- [ ] **Task 9: Create Embedding Generation** (AC: 5)
  - [ ] Create `convex/ai/embeddings.ts`
  - [ ] Install Voyage AI SDK
  - [ ] Add `VOYAGE_API_KEY` to Convex environment
  - [ ] Implement `generateEmbedding` for content chunks
  - [ ] Chunk content to ~500 tokens before embedding

---

## Dev Notes

### Mastra Agent Configuration
[Source: architecture.md#backend-architecture]

```typescript
// agents/corridorResearcher.ts
import { Agent } from "@mastra/core";
import { firecrawlTool } from "./tools/firecrawl";
import { tavilyTool } from "./tools/tavily";
import { redditTool } from "./tools/reddit";
import { perplexityTool } from "./tools/perplexity";

export const corridorResearcher = new Agent({
  name: "CorridorResearcher",
  model: "claude-sonnet-4-20250514",
  instructions: `You are a migration research specialist for TRIBE - The Diaspora Intelligence Network.

Your role is to gather accurate, up-to-date information about migration pathways (corridors).

When researching a corridor (origin → destination):

1. OFFICIAL SOURCES FIRST
   - Check government immigration websites (embassy, visa portals)
   - Look for recent policy changes (last 6 months)
   - Verify visa requirements and processing times

2. COMMUNITY EXPERIENCES
   - Search Reddit for real experiences (r/IWantOut, r/expats, country subs)
   - Find forum discussions (InterNations, Expatica)
   - Look for recent blog posts from migrants

3. PRACTICAL INFORMATION
   - Cost of living comparisons
   - Housing market insights
   - Job market for foreigners
   - Banking and financial setup

4. QUALITY STANDARDS
   - Always cite your sources with URLs
   - Prioritize recent content (< 1 year old)
   - Flag any conflicting information
   - Note the author/username when available

Output structured research that can be synthesized into actionable protocols.`,
  tools: [firecrawlTool, tavilyTool, redditTool, perplexityTool],
});
```

### Firecrawl Tool
[Source: docs/api-tribe.md]

```typescript
// agents/tools/firecrawl.ts
import { createTool } from "@mastra/core";
import FirecrawlApp from "@mendable/firecrawl-js";
import { z } from "zod";

const firecrawl = new FirecrawlApp({
  apiKey: process.env.FIRECRAWL_API_KEY,
});

export const firecrawlTool = createTool({
  name: "firecrawl_scrape",
  description: "Scrape a webpage and extract its content as markdown. Use for forums, blogs, and government sites.",
  schema: z.object({
    url: z.string().url().describe("The URL to scrape"),
    waitForSelector: z.string().optional().describe("CSS selector to wait for (for JS-rendered pages)"),
  }),
  execute: async ({ url, waitForSelector }) => {
    try {
      const result = await firecrawl.scrapeUrl(url, {
        formats: ["markdown"],
        waitFor: waitForSelector ? 5000 : undefined,
      });

      if (!result.success) {
        return { error: `Failed to scrape: ${result.error}` };
      }

      return {
        url,
        title: result.metadata?.title ?? "",
        content: result.markdown ?? "",
        scrapedAt: new Date().toISOString(),
      };
    } catch (error) {
      return { error: `Firecrawl error: ${error.message}` };
    }
  },
});
```

### Tavily Tool
[Source: architecture.md#external-apis]

```typescript
// agents/tools/tavily.ts
import { createTool } from "@mastra/core";
import { tavily } from "@tavily/core";
import { z } from "zod";

const client = tavily({ apiKey: process.env.TAVILY_API_KEY });

export const tavilyTool = createTool({
  name: "tavily_search",
  description: "Search the web for immigration and migration information. Returns relevant URLs and snippets.",
  schema: z.object({
    query: z.string().describe("Search query about migration, visas, or relocation"),
    maxResults: z.number().default(5).describe("Maximum number of results"),
  }),
  execute: async ({ query, maxResults }) => {
    try {
      const response = await client.search(query, {
        maxResults,
        searchDepth: "advanced",
        includeAnswer: true,
      });

      return {
        answer: response.answer,
        results: response.results.map((r) => ({
          title: r.title,
          url: r.url,
          snippet: r.content,
          score: r.score,
        })),
      };
    } catch (error) {
      return { error: `Tavily error: ${error.message}` };
    }
  },
});
```

### Reddit Tool
[Source: PRD#4.3]

```typescript
// agents/tools/reddit.ts
import { createTool } from "@mastra/core";
import { z } from "zod";

export const redditTool = createTool({
  name: "reddit_search",
  description: "Search Reddit for migration experiences and advice. Targets expat and immigration subreddits.",
  schema: z.object({
    query: z.string().describe("Search query"),
    subreddits: z.array(z.string()).default(["IWantOut", "expats", "immigration"]),
    limit: z.number().default(10),
  }),
  execute: async ({ query, subreddits, limit }) => {
    try {
      // Using Reddit's public JSON API (no auth needed for search)
      const results = [];

      for (const subreddit of subreddits) {
        const url = `https://www.reddit.com/r/${subreddit}/search.json?q=${encodeURIComponent(query)}&restrict_sr=1&limit=${limit}&sort=relevance`;

        const response = await fetch(url, {
          headers: { "User-Agent": "TRIBE/1.0" },
        });

        if (!response.ok) continue;

        const data = await response.json();
        const posts = data.data.children.map((child: any) => ({
          title: child.data.title,
          selftext: child.data.selftext?.slice(0, 500),
          author: child.data.author,
          url: `https://reddit.com${child.data.permalink}`,
          score: child.data.score,
          numComments: child.data.num_comments,
          createdAt: new Date(child.data.created_utc * 1000).toISOString(),
          subreddit: child.data.subreddit,
        }));

        results.push(...posts);
      }

      return { posts: results.slice(0, limit) };
    } catch (error) {
      return { error: `Reddit error: ${error.message}` };
    }
  },
});
```

### Perplexity Tool
[Source: architecture.md#external-apis]

```typescript
// agents/tools/perplexity.ts
import { createTool } from "@mastra/core";
import { z } from "zod";

export const perplexityTool = createTool({
  name: "perplexity_query",
  description: "Query Perplexity for real-time policy information and current immigration rules.",
  schema: z.object({
    query: z.string().describe("Question about current immigration policies or rules"),
  }),
  execute: async ({ query }) => {
    try {
      const response = await fetch("https://api.perplexity.ai/chat/completions", {
        method: "POST",
        headers: {
          Authorization: `Bearer ${process.env.PERPLEXITY_API_KEY}`,
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: "llama-3.1-sonar-small-128k-online",
          messages: [
            {
              role: "system",
              content: "You are a helpful assistant that provides accurate, current immigration policy information with citations.",
            },
            { role: "user", content: query },
          ],
        }),
      });

      if (!response.ok) {
        throw new Error(`Perplexity API error: ${response.status}`);
      }

      const data = await response.json();
      return {
        answer: data.choices[0].message.content,
        citations: data.citations ?? [],
      };
    } catch (error) {
      return { error: `Perplexity error: ${error.message}` };
    }
  },
});
```

### Convex Research Action
[Source: architecture.md#backend-architecture]

```typescript
// convex/ai/research.ts
import { action } from "../_generated/server";
import { v } from "convex/values";
import { api } from "../_generated/api";
import { corridorResearcher } from "../../agents/corridorResearcher";

export const researchCorridor = action({
  args: {
    corridorId: v.id("corridors"),
    query: v.optional(v.string()),
  },
  handler: async (ctx, { corridorId, query }) => {
    // Get corridor details
    const corridor = await ctx.runQuery(api.corridors.getCorridor, {
      id: corridorId,
    });

    if (!corridor) {
      throw new Error("Corridor not found");
    }

    const researchQuery = query ??
      `Research migration from ${corridor.origin} to ${corridor.destination}.
       Focus on visa requirements, cost of living, and community experiences.`;

    // Execute Mastra agent
    const result = await corridorResearcher.generate(researchQuery, {
      context: {
        origin: corridor.origin,
        destination: corridor.destination,
        stage: corridor.stage,
      },
    });

    // Store scraped content with embeddings
    if (result.toolResults) {
      for (const toolResult of result.toolResults) {
        if (toolResult.tool === "firecrawl_scrape" && toolResult.content) {
          await ctx.runAction(api.ai.embeddings.storeContent, {
            corridorId,
            url: toolResult.url,
            title: toolResult.title ?? "",
            content: toolResult.content,
            source: detectSource(toolResult.url),
          });
        }
      }
    }

    return {
      response: result.text,
      sources: result.toolResults?.map((t) => t.url).filter(Boolean) ?? [],
      toolsUsed: result.toolResults?.map((t) => t.tool) ?? [],
    };
  },
});

function detectSource(url: string): "reddit" | "forum" | "blog" | "government" | "news" {
  if (url.includes("reddit.com")) return "reddit";
  if (url.includes("nairaland") || url.includes("internations")) return "forum";
  if (url.includes(".gov") || url.includes("embassy") || url.includes("uscis")) return "government";
  return "blog";
}
```

### Embedding Generation
[Source: architecture.md#database-schema]

```typescript
// convex/ai/embeddings.ts
import { action } from "../_generated/server";
import { v } from "convex/values";
import { api } from "../_generated/api";

export const storeContent = action({
  args: {
    corridorId: v.id("corridors"),
    url: v.string(),
    title: v.string(),
    content: v.string(),
    source: v.union(
      v.literal("reddit"),
      v.literal("forum"),
      v.literal("blog"),
      v.literal("government"),
      v.literal("news")
    ),
  },
  handler: async (ctx, { corridorId, url, title, content, source }) => {
    // Check if already stored
    const existing = await ctx.runQuery(api.ingestedContent.getByUrl, { url });
    if (existing) return existing._id;

    // Chunk content (~500 tokens per chunk)
    const chunks = chunkContent(content, 2000); // ~500 tokens ≈ 2000 chars

    // Generate embeddings via Voyage AI
    const embeddings = await generateEmbeddings(chunks);

    // Store each chunk
    for (let i = 0; i < chunks.length; i++) {
      await ctx.runMutation(api.ingestedContent.store, {
        corridorId,
        url: `${url}#chunk-${i}`,
        title: `${title} (${i + 1}/${chunks.length})`,
        content: chunks[i],
        embedding: embeddings[i],
        source,
        metadata: { chunkIndex: i, totalChunks: chunks.length },
        scrapedAt: Date.now(),
        expiresAt: Date.now() + 30 * 24 * 60 * 60 * 1000, // 30 days
      });
    }
  },
});

function chunkContent(content: string, maxLength: number): string[] {
  const chunks: string[] = [];
  const paragraphs = content.split(/\n\n+/);

  let currentChunk = "";
  for (const para of paragraphs) {
    if (currentChunk.length + para.length > maxLength) {
      if (currentChunk) chunks.push(currentChunk.trim());
      currentChunk = para;
    } else {
      currentChunk += "\n\n" + para;
    }
  }
  if (currentChunk) chunks.push(currentChunk.trim());

  return chunks;
}

async function generateEmbeddings(texts: string[]): Promise<number[][]> {
  const response = await fetch("https://api.voyageai.com/v1/embeddings", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${process.env.VOYAGE_API_KEY}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      input: texts,
      model: "voyage-3",
      input_type: "document",
    }),
  });

  const data = await response.json();
  return data.data.map((d: any) => d.embedding);
}
```

### Environment Variables
```bash
# Add to Convex environment
npx convex env set ANTHROPIC_API_KEY=sk-ant-xxx
npx convex env set FIRECRAWL_API_KEY=fc-xxx
npx convex env set TAVILY_API_KEY=tvly-xxx
npx convex env set PERPLEXITY_API_KEY=pplx-xxx
npx convex env set VOYAGE_API_KEY=voyage-xxx
```

### File Structure
```
agents/
├── corridorResearcher.ts      # Main research agent
└── tools/
    ├── firecrawl.ts           # Web scraping
    ├── tavily.ts              # Web search
    ├── reddit.ts              # Reddit search
    └── perplexity.ts          # Real-time queries

convex/
└── ai/
    ├── research.ts            # Research action
    └── embeddings.ts          # Embedding generation
```

### Dependencies from Previous Stories
- Story 2.1: Corridors table, IngestedContent table with vector index
- Story 2.2: API caching patterns

---

## Testing

### Test Scenarios

1. **Agent Execution**
   - [ ] Create test corridor (NG → DE)
   - [ ] Call `researchCorridor` action
   - [ ] Verify agent returns structured response
   - [ ] Check IngestedContent table for stored data

2. **Individual Tools**
   - [ ] Firecrawl: Scrape known URL, verify markdown output
   - [ ] Tavily: Search "Nigeria to Germany visa", verify results
   - [ ] Reddit: Search in r/IWantOut, verify posts returned
   - [ ] Perplexity: Query current visa policy, verify citations

3. **Embedding Storage**
   - [ ] Store content via storeContent action
   - [ ] Verify embedding array has 1024 dimensions
   - [ ] Verify chunking works for long content

4. **Error Handling**
   - [ ] Test with invalid API key
   - [ ] Test with rate-limited API
   - [ ] Verify partial results returned

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-21 | 1.0 | Initial story draft | Bob (SM) |

---

## Dev Agent Record

### Agent Model Used
*To be filled by Dev Agent*

### Debug Log References
*To be filled by Dev Agent*

### Completion Notes List
*To be filled by Dev Agent*

### File List
*To be filled by Dev Agent*

---

## QA Results
*To be filled by QA Agent*
