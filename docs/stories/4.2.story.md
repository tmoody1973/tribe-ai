# Story 4.2: RAG Retrieval with Voyage AI

## Status

**Completed**

---

## Story

**As a** system,
**I want** to retrieve relevant content for user queries,
**so that** answers are grounded in actual community experiences.

---

## Acceptance Criteria

1. Voyage AI client configured for embedding generation
2. User query embedded and similarity search performed
3. Top 5-10 relevant chunks retrieved from Convex vector store
4. Chunks include metadata: source URL, author, date, engagement
5. Retrieval filtered by user's corridor for relevance
6. Fallback to broader search if corridor-specific results insufficient
7. Retrieval latency <1 second

---

## Tasks / Subtasks

- [x] **Task 1: Configure Voyage AI Client** (AC: 1)
  - [x] Add `VOYAGE_API_KEY` to Convex environment
  - [x] Create `lib/voyage.ts` with embed function
  - [x] Configure voyage-3 model (1024 dimensions)
  - [x] Add error handling for API calls

- [x] **Task 2: Create Query Embedding Action** (AC: 2)
  - [x] Create `convex/ai/embeddings.ts` (already exists)
  - [x] Implement `generateQueryEmbedding` action
  - [x] Batch embedding with `storeContentWithEmbedding`

- [x] **Task 3: Implement Vector Search** (AC: 2, 3)
  - [x] Create `convex/ai/searchActions.ts`
  - [x] Implement `searchRelevantContent` action
  - [x] Use Convex vectorSearch with `by_embedding` index
  - [x] Return top 10 results by similarity score

- [x] **Task 4: Filter by Corridor** (AC: 5)
  - [x] Add corridorId filter to vector search
  - [x] Use Convex filter fields in vector index
  - [x] Validate corridor filter works

- [x] **Task 5: Include Metadata in Results** (AC: 4)
  - [x] Return content with full metadata
  - [x] Include source URL, author, date
  - [x] Format results for synthesis with SearchResult interface

- [x] **Task 6: Implement Fallback Search** (AC: 6)
  - [x] Check result count after corridor filter (< minResults)
  - [x] If insufficient, search globally without corridor filter
  - [x] Mark results as `isCorridorSpecific: true/false`
  - [x] Log fallback events with timing

- [x] **Task 7: Optimize for Latency** (AC: 7)
  - [x] Add timing logs to measure latency
  - [x] Separate embedding and search timing
  - [x] Log total search completion time

---

## Dev Notes

### Voyage AI Client
[Source: architecture.md#external-services]

```typescript
// lib/voyage.ts
interface VoyageEmbeddingResponse {
  data: Array<{
    embedding: number[];
    index: number;
  }>;
  model: string;
  usage: {
    total_tokens: number;
  };
}

const VOYAGE_API_URL = "https://api.voyageai.com/v1/embeddings";
const VOYAGE_MODEL = "voyage-3";
const EMBEDDING_DIMENSIONS = 1024;

export async function embedText(text: string): Promise<number[]> {
  const apiKey = process.env.VOYAGE_API_KEY;
  if (!apiKey) {
    throw new Error("VOYAGE_API_KEY not configured");
  }

  const response = await fetch(VOYAGE_API_URL, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: VOYAGE_MODEL,
      input: text,
      input_type: "query",
    }),
  });

  if (!response.ok) {
    throw new Error(`Voyage API error: ${response.status}`);
  }

  const data: VoyageEmbeddingResponse = await response.json();
  return data.data[0].embedding;
}

export async function embedTexts(texts: string[]): Promise<number[][]> {
  const apiKey = process.env.VOYAGE_API_KEY;
  if (!apiKey) {
    throw new Error("VOYAGE_API_KEY not configured");
  }

  const response = await fetch(VOYAGE_API_URL, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: VOYAGE_MODEL,
      input: texts,
      input_type: "document",
    }),
  });

  if (!response.ok) {
    throw new Error(`Voyage API error: ${response.status}`);
  }

  const data: VoyageEmbeddingResponse = await response.json();
  return data.data.map((d) => d.embedding);
}
```

### Embedding Action
```typescript
// convex/ai/embeddings.ts
import { action } from "../_generated/server";
import { v } from "convex/values";
import { embedText, embedTexts } from "../../lib/voyage";

export const embedQuery = action({
  args: {
    query: v.string(),
  },
  handler: async (ctx, { query }): Promise<number[]> => {
    const startTime = Date.now();

    try {
      const embedding = await embedText(query);

      console.log(`Query embedded in ${Date.now() - startTime}ms`);
      return embedding;
    } catch (error) {
      console.error("Embedding error:", error);
      throw error;
    }
  },
});

export const embedDocuments = action({
  args: {
    documents: v.array(v.string()),
  },
  handler: async (ctx, { documents }): Promise<number[][]> => {
    const startTime = Date.now();

    try {
      // Batch in chunks of 100 for API limits
      const batchSize = 100;
      const allEmbeddings: number[][] = [];

      for (let i = 0; i < documents.length; i += batchSize) {
        const batch = documents.slice(i, i + batchSize);
        const embeddings = await embedTexts(batch);
        allEmbeddings.push(...embeddings);
      }

      console.log(`${documents.length} documents embedded in ${Date.now() - startTime}ms`);
      return allEmbeddings;
    } catch (error) {
      console.error("Batch embedding error:", error);
      throw error;
    }
  },
});
```

### Vector Search Action
```typescript
// convex/ai/search.ts
import { action } from "../_generated/server";
import { v } from "convex/values";
import { api, internal } from "../_generated/api";
import { Id } from "../_generated/dataModel";

interface SearchResult {
  id: Id<"ingestedContent">;
  content: string;
  score: number;
  metadata: {
    url: string;
    author?: string;
    publishedAt?: number;
    engagement?: number;
    corridorId: Id<"corridors">;
  };
  isCorridorSpecific: boolean;
}

export const searchRelevantContent = action({
  args: {
    query: v.string(),
    corridorId: v.optional(v.id("corridors")),
    limit: v.optional(v.number()),
    minResults: v.optional(v.number()),
  },
  handler: async (
    ctx,
    { query, corridorId, limit = 10, minResults = 3 }
  ): Promise<SearchResult[]> => {
    const startTime = Date.now();

    // Step 1: Embed the query
    const queryEmbedding = await ctx.runAction(api.ai.embeddings.embedQuery, {
      query,
    });

    // Step 2: Search with corridor filter
    let results = await ctx.runQuery(internal.ai.search.vectorSearch, {
      embedding: queryEmbedding,
      corridorId,
      limit,
    });

    let isCorridorSpecific = true;

    // Step 3: Fallback to broader search if needed
    if (results.length < minResults && corridorId) {
      console.log(
        `Only ${results.length} corridor-specific results, falling back to global search`
      );

      const globalResults = await ctx.runQuery(internal.ai.search.vectorSearch, {
        embedding: queryEmbedding,
        corridorId: undefined,
        limit,
      });

      // Merge results, prioritizing corridor-specific
      const corridorIds = new Set(results.map((r) => r._id));
      const additionalResults = globalResults.filter(
        (r) => !corridorIds.has(r._id)
      );

      results = [
        ...results.map((r) => ({ ...r, isCorridorSpecific: true })),
        ...additionalResults
          .slice(0, limit - results.length)
          .map((r) => ({ ...r, isCorridorSpecific: false })),
      ];
      isCorridorSpecific = false;
    }

    const latency = Date.now() - startTime;
    console.log(`Search completed in ${latency}ms, ${results.length} results`);

    // Format results with metadata
    return results.map((r) => ({
      id: r._id,
      content: r.content,
      score: r._score,
      metadata: {
        url: r.url,
        author: r.metadata?.author,
        publishedAt: r.metadata?.publishedAt,
        engagement: r.metadata?.engagement,
        corridorId: r.corridorId,
      },
      isCorridorSpecific: r.isCorridorSpecific ?? isCorridorSpecific,
    }));
  },
});
```

### Internal Vector Search Query
```typescript
// convex/ai/search.ts (internal query)
import { internalQuery } from "../_generated/server";
import { v } from "convex/values";

export const vectorSearch = internalQuery({
  args: {
    embedding: v.array(v.float64()),
    corridorId: v.optional(v.id("corridors")),
    limit: v.number(),
  },
  handler: async (ctx, { embedding, corridorId, limit }) => {
    // Use Convex vector search with optional corridor filter
    const results = await ctx.db
      .query("ingestedContent")
      .withSearchIndex("by_embedding", (q) => {
        let search = q.vectorSearch("embedding", embedding);
        if (corridorId) {
          search = search.filter((f) => f.eq("corridorId", corridorId));
        }
        return search;
      })
      .take(limit);

    return results;
  },
});
```

### Convex Schema for Vector Index
[Source: Story 2.1]

```typescript
// convex/schema.ts - ingestedContent table with vector index
ingestedContent: defineTable({
  corridorId: v.id("corridors"),
  url: v.string(),
  content: v.string(),
  embedding: v.optional(v.array(v.float64())),
  metadata: v.optional(v.object({
    author: v.optional(v.string()),
    publishedAt: v.optional(v.number()),
    engagement: v.optional(v.number()),
    title: v.optional(v.string()),
  })),
  scrapedAt: v.number(),
})
  .index("by_corridor", ["corridorId"])
  .vectorIndex("by_embedding", {
    vectorField: "embedding",
    dimensions: 1024,
    filterFields: ["corridorId"],
  }),
```

### File Structure
```
lib/
└── voyage.ts             # Voyage AI client

convex/
└── ai/
    ├── embeddings.ts     # Embedding actions
    └── search.ts         # Vector search action
```

### Dependencies from Previous Stories
- Story 2.1: ingestedContent table with vector index
- Story 2.3: Ingested content with embeddings

---

## Testing

### Test Scenarios

1. **Query Embedding**
   - [ ] Query embedded successfully
   - [ ] Returns 1024-dimension vector
   - [ ] Handles special characters

2. **Vector Search**
   - [ ] Returns relevant results
   - [ ] Results sorted by similarity score
   - [ ] Limit respected

3. **Corridor Filter**
   - [ ] Corridor-specific results returned first
   - [ ] Filter reduces result set appropriately
   - [ ] Different corridors return different results

4. **Fallback Search**
   - [ ] Triggers when < 3 corridor results
   - [ ] Returns mixed results with flags
   - [ ] Doesn't duplicate results

5. **Latency**
   - [ ] Total search < 1 second
   - [ ] Embedding < 200ms
   - [ ] Vector query < 500ms

6. **Metadata**
   - [ ] Source URL included
   - [ ] Author included when available
   - [ ] Date and engagement returned

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-21 | 1.0 | Initial story draft | Bob (SM) |

---

## Dev Agent Record

**Completed:** 2025-12-22

### Files Created
- `lib/voyage.ts` - Voyage AI client with embedText and embedTexts functions
- `convex/ai/search.ts` - Internal queries for getContentById, getContentByIds
- `convex/ai/searchActions.ts` - Vector search actions with corridor filter and fallback

### Implementation Notes
- Used Voyage AI voyage-3 model with 1024 dimensions
- Vector search uses ctx.vectorSearch() which is only available in actions
- Corridor filter uses vectorIndex filterFields for efficient filtering
- Fallback to global search when < minResults (default 3) corridor-specific
- Results marked with isCorridorSpecific flag for transparency
- Timing logs added for embedding, search, and total latency

### Key Functions
- `searchRelevantContent` - Main RAG retrieval with fallback
- `getSimilarContent` - Find related content for recommendations
- `generateQueryEmbedding` - Embed search queries (already existed)
- `storeContentWithEmbedding` - Store ingested content with embeddings (already existed)

### Build Status
- TypeScript: ✅ Passed
- Next.js build: ✅ Passed

---

## QA Results
*To be filled by QA Agent*
