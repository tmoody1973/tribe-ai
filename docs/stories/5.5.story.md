# Story 5.5: Voice Agent TTS Response

## Status

**Draft**

---

## Story

**As a** user,
**I want** to hear answers spoken back to me,
**so that** I can have a fully hands-free conversation.

---

## Acceptance Criteria

1. Q&A responses optionally spoken via ElevenLabs TTS
2. "Listen" button on each response to trigger TTS
3. Auto-speak toggle in settings for hands-free mode
4. Voice response uses same voice as audio briefings (consistency)
5. Response speaks in user's selected language
6. Speaking indicator shows when audio is playing
7. Interrupt capability to stop playback

---

## Tasks / Subtasks

- [ ] **Task 1: Create Response TTS Action** (AC: 1, 5)
  - [ ] Create `convex/ai/responseTTS.ts`
  - [ ] Implement on-demand TTS for responses
  - [ ] Use same voice as briefings
  - [ ] Handle long responses

- [ ] **Task 2: Add Listen Button to Messages** (AC: 2)
  - [ ] Add "Listen" button to assistant messages
  - [ ] Icon-based for compact display
  - [ ] Loading state during generation

- [ ] **Task 3: Create Auto-Speak Setting** (AC: 3)
  - [ ] Add `autoSpeak` to user preferences
  - [ ] Toggle in settings page
  - [ ] Persist preference

- [ ] **Task 4: Create Speaking Indicator** (AC: 6)
  - [ ] Create `components/chat/SpeakingIndicator.tsx`
  - [ ] Show on active message
  - [ ] Animate during playback

- [ ] **Task 5: Implement Interrupt** (AC: 7)
  - [ ] Stop button appears during playback
  - [ ] Click stops audio immediately
  - [ ] Keyboard shortcut (Escape)

- [ ] **Task 6: Create Voice Response Hook** (AC: 1, 2, 4)
  - [ ] Create `hooks/useVoiceResponse.ts`
  - [ ] Manage TTS generation and playback
  - [ ] Queue management for multiple

- [ ] **Task 7: Integrate with Chat** (AC: 1-7)
  - [ ] Enhance ChatMessage component
  - [ ] Auto-speak new responses if enabled
  - [ ] Track speaking state globally

---

## Dev Notes

### Response TTS Action
[Source: Story 5.2 patterns]

```typescript
// convex/ai/responseTTS.ts
import { action } from "../_generated/server";
import { v } from "convex/values";
import { textToSpeech } from "../../lib/elevenlabs";

export const generateResponseAudio = action({
  args: {
    text: v.string(),
    language: v.string(),
  },
  handler: async (ctx, { text, language }) => {
    // Truncate very long responses for TTS
    const maxLength = 2000;
    const truncatedText =
      text.length > maxLength
        ? text.substring(0, maxLength) + "..."
        : text;

    // Generate audio
    const audioBuffer = await textToSpeech(truncatedText, language);

    // Store in Convex
    const blob = new Blob([audioBuffer], { type: "audio/mpeg" });
    const storageId = await ctx.storage.store(blob);
    const audioUrl = await ctx.storage.getUrl(storageId);

    return { audioUrl };
  },
});
```

### Voice Response Hook
```typescript
// hooks/useVoiceResponse.ts
"use client";

import { useState, useRef, useCallback } from "react";
import { useAction } from "convex/react";
import { api } from "@/convex/_generated/api";

interface UseVoiceResponseReturn {
  speakingMessageId: string | null;
  isLoading: boolean;
  speak: (messageId: string, text: string, language: string) => Promise<void>;
  stop: () => void;
}

export function useVoiceResponse(): UseVoiceResponseReturn {
  const [speakingMessageId, setSpeakingMessageId] = useState<string | null>(null);
  const [isLoading, setIsLoading] = useState(false);
  const audioRef = useRef<HTMLAudioElement | null>(null);

  const generateAudio = useAction(api.ai.responseTTS.generateResponseAudio);

  const speak = useCallback(
    async (messageId: string, text: string, language: string) => {
      // Stop any current playback
      if (audioRef.current) {
        audioRef.current.pause();
        audioRef.current = null;
      }

      setIsLoading(true);
      setSpeakingMessageId(messageId);

      try {
        const { audioUrl } = await generateAudio({ text, language });

        if (!audioUrl) {
          throw new Error("No audio URL returned");
        }

        const audio = new Audio(audioUrl);
        audioRef.current = audio;

        audio.onended = () => {
          setSpeakingMessageId(null);
          audioRef.current = null;
        };

        audio.onerror = () => {
          setSpeakingMessageId(null);
          audioRef.current = null;
        };

        await audio.play();
      } catch (error) {
        console.error("TTS error:", error);
        setSpeakingMessageId(null);
      } finally {
        setIsLoading(false);
      }
    },
    [generateAudio]
  );

  const stop = useCallback(() => {
    if (audioRef.current) {
      audioRef.current.pause();
      audioRef.current = null;
    }
    setSpeakingMessageId(null);
  }, []);

  return {
    speakingMessageId,
    isLoading,
    speak,
    stop,
  };
}
```

### Chat Message with Listen Button
```typescript
// components/chat/ChatMessage.tsx - Updated
"use client";

import { useState } from "react";
import { Volume2, VolumeX, Loader } from "lucide-react";
import { SourcesList } from "./SourcesList";
import { SpeakingIndicator } from "./SpeakingIndicator";
import ReactMarkdown from "react-markdown";

interface ChatMessageProps {
  id: string;
  role: "user" | "assistant";
  content: string;
  sources?: Array<{ index: number; url: string; type: "community" | "perplexity" }>;
  timestamp?: number;
  language: string;
  // Voice props
  isSpeaking?: boolean;
  isLoadingAudio?: boolean;
  onSpeak?: (id: string, text: string, language: string) => void;
  onStopSpeaking?: () => void;
}

export function ChatMessage({
  id,
  role,
  content,
  sources,
  timestamp,
  language,
  isSpeaking = false,
  isLoadingAudio = false,
  onSpeak,
  onStopSpeaking,
}: ChatMessageProps) {
  const isAssistant = role === "assistant";

  const handleListen = () => {
    if (isSpeaking) {
      onStopSpeaking?.();
    } else {
      onSpeak?.(id, content, language);
    }
  };

  return (
    <div
      className={`
        relative p-4 border-2 border-black shadow-[2px_2px_0_0_#000]
        ${isAssistant ? "bg-gray-50" : "bg-yellow-50"}
        ${isAssistant ? "mr-8" : "ml-8"}
      `}
    >
      {/* Speaking Indicator */}
      {isSpeaking && <SpeakingIndicator />}

      {/* Role Label */}
      <div className="flex items-center justify-between mb-2">
        <span className="text-xs font-bold text-gray-500">
          {isAssistant ? "TRIBE" : "You"}
        </span>

        {/* Listen Button (assistant only) */}
        {isAssistant && onSpeak && (
          <button
            onClick={handleListen}
            disabled={isLoadingAudio}
            className={`
              p-1.5 border-2 border-black
              ${isSpeaking ? "bg-red-100 text-red-600" : "hover:bg-gray-100"}
            `}
            title={isSpeaking ? "Stop" : "Listen"}
          >
            {isLoadingAudio ? (
              <Loader size={16} className="animate-spin" />
            ) : isSpeaking ? (
              <VolumeX size={16} />
            ) : (
              <Volume2 size={16} />
            )}
          </button>
        )}
      </div>

      {/* Content */}
      <div className="prose prose-sm max-w-none">
        <ReactMarkdown>{content}</ReactMarkdown>
      </div>

      {/* Sources (assistant only) */}
      {isAssistant && sources && sources.length > 0 && (
        <SourcesList sources={sources} />
      )}

      {/* Timestamp */}
      {timestamp && (
        <div className="text-xs text-gray-400 mt-2">
          {new Date(timestamp).toLocaleTimeString()}
        </div>
      )}
    </div>
  );
}
```

### Speaking Indicator Component
```typescript
// components/chat/SpeakingIndicator.tsx
"use client";

export function SpeakingIndicator() {
  return (
    <div className="absolute -top-2 -right-2 flex items-center gap-1 px-2 py-1 bg-green-500 text-white text-xs font-bold border-2 border-black">
      <div className="flex items-center gap-0.5">
        {[...Array(3)].map((_, i) => (
          <div
            key={i}
            className="w-1 bg-white rounded-full animate-pulse"
            style={{
              height: `${8 + (i % 2) * 4}px`,
              animationDelay: `${i * 0.15}s`,
            }}
          />
        ))}
      </div>
      <span>Speaking</span>
    </div>
  );
}
```

### Auto-Speak Setting
```typescript
// components/settings/VoiceSettings.tsx
"use client";

import { useMutation, useQuery } from "convex/react";
import { api } from "@/convex/_generated/api";
import { useTranslations } from "next-intl";

export function VoiceSettings() {
  const t = useTranslations("settings.voice");
  const profile = useQuery(api.users.getProfile);
  const updateProfile = useMutation(api.users.updateProfile);

  const handleAutoSpeakToggle = () => {
    updateProfile({
      autoSpeak: !profile?.autoSpeak,
    });
  };

  return (
    <div className="border-4 border-black bg-white p-4 shadow-[4px_4px_0_0_#000]">
      <h3 className="text-lg font-bold mb-4 border-b-2 border-black pb-2">
        {t("title")}
      </h3>

      <div className="space-y-4">
        {/* Auto-Speak Toggle */}
        <div className="flex items-center justify-between">
          <div>
            <p className="font-bold">{t("autoSpeak")}</p>
            <p className="text-sm text-gray-600">{t("autoSpeakDescription")}</p>
          </div>
          <button
            onClick={handleAutoSpeakToggle}
            className={`
              w-14 h-8 border-4 border-black relative
              ${profile?.autoSpeak ? "bg-green-500" : "bg-gray-200"}
            `}
          >
            <div
              className={`
                absolute top-0.5 w-6 h-6 bg-white border-2 border-black
                transition-all
                ${profile?.autoSpeak ? "right-0.5" : "left-0.5"}
              `}
            />
          </button>
        </div>
      </div>
    </div>
  );
}
```

### Updated User Schema
```typescript
// convex/schema.ts - Add to users table
users: defineTable({
  // ... existing fields ...
  autoSpeak: v.optional(v.boolean()), // Auto-speak responses
})
```

### Chat Window with Voice Integration
```typescript
// components/chat/ChatWindowWithVoice.tsx
"use client";

import { useEffect } from "react";
import { useQuery } from "convex/react";
import { api } from "@/convex/_generated/api";
import { Id } from "@/convex/_generated/dataModel";
import { ChatMessage } from "./ChatMessage";
import { ChatInput } from "./ChatInput";
import { useVoiceResponse } from "@/hooks/useVoiceResponse";

interface ChatWindowWithVoiceProps {
  corridorId: Id<"corridors">;
}

export function ChatWindowWithVoice({ corridorId }: ChatWindowWithVoiceProps) {
  const profile = useQuery(api.users.getProfile);
  const messages = useQuery(api.chat.getMessages, { corridorId });
  const { speakingMessageId, isLoading, speak, stop } = useVoiceResponse();

  const language = profile?.language ?? "en";
  const autoSpeak = profile?.autoSpeak ?? false;

  // Auto-speak new assistant messages
  useEffect(() => {
    if (!autoSpeak || !messages || messages.length === 0) return;

    const lastMessage = messages[messages.length - 1];
    if (
      lastMessage.role === "assistant" &&
      !speakingMessageId &&
      // Only auto-speak messages from the last 5 seconds
      Date.now() - lastMessage.createdAt < 5000
    ) {
      speak(lastMessage._id, lastMessage.content, language);
    }
  }, [messages, autoSpeak, speakingMessageId, speak, language]);

  // Stop on Escape key
  useEffect(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      if (e.key === "Escape" && speakingMessageId) {
        stop();
      }
    };

    window.addEventListener("keydown", handleKeyDown);
    return () => window.removeEventListener("keydown", handleKeyDown);
  }, [speakingMessageId, stop]);

  return (
    <div className="flex flex-col h-full border-4 border-black">
      {/* Messages */}
      <div className="flex-1 overflow-y-auto p-4 space-y-4">
        {messages?.map((message) => (
          <ChatMessage
            key={message._id}
            id={message._id}
            role={message.role}
            content={message.content}
            timestamp={message.createdAt}
            language={language}
            isSpeaking={speakingMessageId === message._id}
            isLoadingAudio={isLoading && speakingMessageId === message._id}
            onSpeak={speak}
            onStopSpeaking={stop}
          />
        ))}
      </div>

      {/* Input */}
      <ChatInput
        onSend={() => {}}
        language={language}
      />
    </div>
  );
}
```

### Translation Keys
```json
// messages/en.json - Add voice response keys
{
  "settings": {
    "voice": {
      "title": "Voice Settings",
      "autoSpeak": "Auto-speak responses",
      "autoSpeakDescription": "Automatically read assistant responses aloud"
    }
  }
}
```

### File Structure
```
hooks/
└── useVoiceResponse.ts   # Voice playback hook

components/
├── chat/
│   ├── ChatMessage.tsx   # Updated with listen button
│   ├── SpeakingIndicator.tsx
│   └── ChatWindowWithVoice.tsx
└── settings/
    └── VoiceSettings.tsx

convex/
└── ai/
    └── responseTTS.ts    # Response TTS action
```

### Dependencies from Previous Stories
- Story 4.1: Chat interface
- Story 5.2: ElevenLabs TTS client
- Story 5.4: Voice input patterns

---

## Testing

### Test Scenarios

1. **Listen Button**
   - [ ] Button appears on assistant messages
   - [ ] Click triggers TTS generation
   - [ ] Loading state shown

2. **Audio Playback**
   - [ ] Audio plays after generation
   - [ ] Speaking indicator visible
   - [ ] Audio completes normally

3. **Stop/Interrupt**
   - [ ] Stop button stops audio
   - [ ] Escape key stops audio
   - [ ] State resets properly

4. **Auto-Speak**
   - [ ] Toggle in settings works
   - [ ] New responses spoken automatically
   - [ ] Preference persisted

5. **Voice Consistency**
   - [ ] Same voice as briefings
   - [ ] Works in all languages
   - [ ] Natural sounding

6. **Edge Cases**
   - [ ] Long responses truncated
   - [ ] Multiple rapid clicks handled
   - [ ] Navigation doesn't crash

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-21 | 1.0 | Initial story draft | Bob (SM) |

---

## Dev Agent Record

*To be filled by Dev Agent*

---

## QA Results
*To be filled by QA Agent*
