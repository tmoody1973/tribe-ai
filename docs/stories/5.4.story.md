# Story 5.4: Voice Agent STT Input

## Status

**Done**

---

## Story

**As a** user,
**I want** to ask questions using my voice,
**so that** I can interact with TRIBE hands-free.

---

## Acceptance Criteria

1. Voice input button on Q&A interface
2. ElevenLabs STT converts speech to text
3. Push-to-talk interaction model (hold to record)
4. Visual feedback during recording (waveform or indicator)
5. Transcribed text shown before sending
6. User can edit transcription before submitting
7. Voice input works in user's selected language

---

## Tasks / Subtasks

- [x] **Task 1: Create ElevenLabs STT Client** (AC: 2)
  - [x] Add STT endpoint to `lib/elevenlabs.ts`
  - [x] Handle audio format (webm/mp3)
  - [x] Return transcribed text

- [x] **Task 2: Create Voice Input Button** (AC: 1, 3)
  - [x] Create `components/chat/VoiceInputButton.tsx`
  - [x] Implement push-to-talk (hold to record)
  - [x] Use Web Audio API for recording
  - [x] Handle browser permissions

- [x] **Task 3: Add Recording Indicator** (AC: 4)
  - [x] Create `components/chat/RecordingIndicator.tsx`
  - [x] Animate waveform or pulsing icon
  - [x] Show recording duration
  - [x] Red color to indicate recording

- [x] **Task 4: Create Transcription Preview** (AC: 5)
  - [x] Show transcribed text in preview area
  - [x] Loading state during transcription
  - [x] Handle transcription errors

- [x] **Task 5: Enable Transcription Editing** (AC: 6)
  - [x] Editable text field with transcription
  - [x] "Send" and "Cancel" buttons
  - [x] Re-record option

- [x] **Task 6: Add Language Support** (AC: 7)
  - [x] Pass user's language to STT API
  - [x] Configure language hints
  - [x] Handle multilingual output

- [x] **Task 7: Integrate with Chat** (AC: 1-7)
  - [x] Add voice button to ChatWindow
  - [x] Submit transcription via CopilotKit
  - [x] Handle voice input flow

---

## Dev Notes

### ElevenLabs STT Client
[Source: architecture.md#external-services]

```typescript
// lib/elevenlabs.ts - Add STT function

interface STTResponse {
  text: string;
  language_code: string;
}

export async function speechToText(
  audioBlob: Blob,
  languageHint?: string
): Promise<{ text: string; language: string }> {
  const apiKey = process.env.NEXT_PUBLIC_ELEVENLABS_API_KEY;
  if (!apiKey) {
    throw new Error("ELEVENLABS_API_KEY not configured");
  }

  const formData = new FormData();
  formData.append("audio", audioBlob);
  if (languageHint) {
    formData.append("language_code", languageHint);
  }

  const response = await fetch(
    "https://api.elevenlabs.io/v1/speech-to-text",
    {
      method: "POST",
      headers: {
        "xi-api-key": apiKey,
      },
      body: formData,
    }
  );

  if (!response.ok) {
    throw new Error(`STT API error: ${response.status}`);
  }

  const data: STTResponse = await response.json();
  return {
    text: data.text,
    language: data.language_code,
  };
}
```

### Voice Input Button Component
```typescript
// components/chat/VoiceInputButton.tsx
"use client";

import { useState, useRef, useCallback } from "react";
import { Mic, MicOff } from "lucide-react";
import { useTranslations } from "next-intl";
import { RecordingIndicator } from "./RecordingIndicator";

interface VoiceInputButtonProps {
  onTranscription: (text: string) => void;
  language: string;
  disabled?: boolean;
}

export function VoiceInputButton({
  onTranscription,
  language,
  disabled = false,
}: VoiceInputButtonProps) {
  const t = useTranslations("chat.voice");
  const [isRecording, setIsRecording] = useState(false);
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [error, setError] = useState<string | null>(null);

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const startTimeRef = useRef<number>(0);

  const startRecording = useCallback(async () => {
    try {
      setError(null);
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: "audio/webm",
      });

      mediaRecorderRef.current = mediaRecorder;
      chunksRef.current = [];
      startTimeRef.current = Date.now();

      mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunksRef.current.push(e.data);
        }
      };

      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(chunksRef.current, { type: "audio/webm" });

        // Stop all tracks
        stream.getTracks().forEach((track) => track.stop());

        // Transcribe
        setIsTranscribing(true);
        try {
          const { speechToText } = await import("@/lib/elevenlabs");
          const result = await speechToText(audioBlob, language);
          onTranscription(result.text);
        } catch (err) {
          setError(t("transcriptionFailed"));
          console.error("STT error:", err);
        } finally {
          setIsTranscribing(false);
        }
      };

      mediaRecorder.start();
      setIsRecording(true);
    } catch (err) {
      setError(t("microphoneError"));
      console.error("Microphone error:", err);
    }
  }, [language, onTranscription, t]);

  const stopRecording = useCallback(() => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  }, [isRecording]);

  return (
    <div className="relative">
      {/* Recording Indicator */}
      {isRecording && (
        <RecordingIndicator startTime={startTimeRef.current} />
      )}

      {/* Voice Button */}
      <button
        onMouseDown={startRecording}
        onMouseUp={stopRecording}
        onMouseLeave={stopRecording}
        onTouchStart={startRecording}
        onTouchEnd={stopRecording}
        disabled={disabled || isTranscribing}
        className={`
          p-3 border-4 border-black font-bold
          transition-all duration-150
          ${isRecording ? "bg-red-500 text-white scale-110" : "bg-white hover:bg-gray-100"}
          ${disabled ? "opacity-50 cursor-not-allowed" : ""}
        `}
        title={t("holdToSpeak")}
      >
        {isTranscribing ? (
          <div className="animate-spin">⏳</div>
        ) : isRecording ? (
          <Mic size={24} className="animate-pulse" />
        ) : (
          <Mic size={24} />
        )}
      </button>

      {/* Error Message */}
      {error && (
        <div className="absolute bottom-full left-0 mb-2 p-2 bg-red-100 border-2 border-red-400 text-red-700 text-xs whitespace-nowrap">
          {error}
        </div>
      )}
    </div>
  );
}
```

### Recording Indicator Component
```typescript
// components/chat/RecordingIndicator.tsx
"use client";

import { useState, useEffect } from "react";
import { useTranslations } from "next-intl";

interface RecordingIndicatorProps {
  startTime: number;
}

export function RecordingIndicator({ startTime }: RecordingIndicatorProps) {
  const t = useTranslations("chat.voice");
  const [duration, setDuration] = useState(0);

  useEffect(() => {
    const interval = setInterval(() => {
      setDuration(Math.floor((Date.now() - startTime) / 1000));
    }, 100);

    return () => clearInterval(interval);
  }, [startTime]);

  const formatDuration = (seconds: number): string => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins}:${secs.toString().padStart(2, "0")}`;
  };

  return (
    <div className="absolute -top-16 left-1/2 -translate-x-1/2 flex items-center gap-2 px-4 py-2 bg-red-500 text-white border-4 border-black shadow-[4px_4px_0_0_#000]">
      {/* Waveform Animation */}
      <div className="flex items-center gap-0.5">
        {[...Array(5)].map((_, i) => (
          <div
            key={i}
            className="w-1 bg-white rounded-full animate-pulse"
            style={{
              height: `${12 + Math.random() * 12}px`,
              animationDelay: `${i * 0.1}s`,
            }}
          />
        ))}
      </div>

      {/* Duration */}
      <span className="font-mono font-bold">{formatDuration(duration)}</span>

      {/* Label */}
      <span className="text-xs">{t("recording")}</span>
    </div>
  );
}
```

### Transcription Preview Component
```typescript
// components/chat/TranscriptionPreview.tsx
"use client";

import { useState, useEffect } from "react";
import { useTranslations } from "next-intl";
import { Send, X, RotateCcw } from "lucide-react";

interface TranscriptionPreviewProps {
  text: string;
  onSend: (text: string) => void;
  onCancel: () => void;
  onReRecord: () => void;
}

export function TranscriptionPreview({
  text,
  onSend,
  onCancel,
  onReRecord,
}: TranscriptionPreviewProps) {
  const t = useTranslations("chat.voice");
  const [editedText, setEditedText] = useState(text);

  useEffect(() => {
    setEditedText(text);
  }, [text]);

  return (
    <div className="border-4 border-black bg-yellow-50 p-4 shadow-[4px_4px_0_0_#000]">
      <div className="text-xs font-bold text-gray-500 mb-2">
        {t("transcription")}
      </div>

      {/* Editable Transcription */}
      <textarea
        value={editedText}
        onChange={(e) => setEditedText(e.target.value)}
        className="w-full p-2 border-2 border-black resize-none"
        rows={3}
      />

      {/* Actions */}
      <div className="flex gap-2 mt-3">
        <button
          onClick={() => onSend(editedText)}
          className="flex-1 py-2 bg-black text-white font-bold flex items-center justify-center gap-2"
        >
          <Send size={16} />
          {t("send")}
        </button>
        <button
          onClick={onReRecord}
          className="p-2 border-2 border-black hover:bg-gray-100"
          title={t("reRecord")}
        >
          <RotateCcw size={20} />
        </button>
        <button
          onClick={onCancel}
          className="p-2 border-2 border-black hover:bg-gray-100"
          title={t("cancel")}
        >
          <X size={20} />
        </button>
      </div>
    </div>
  );
}
```

### Updated Chat Input with Voice
```typescript
// components/chat/ChatInput.tsx
"use client";

import { useState } from "react";
import { Send } from "lucide-react";
import { useTranslations } from "next-intl";
import { VoiceInputButton } from "./VoiceInputButton";
import { TranscriptionPreview } from "./TranscriptionPreview";

interface ChatInputProps {
  onSend: (message: string) => void;
  disabled?: boolean;
  language: string;
}

export function ChatInput({ onSend, disabled = false, language }: ChatInputProps) {
  const t = useTranslations("chat");
  const [message, setMessage] = useState("");
  const [transcription, setTranscription] = useState<string | null>(null);

  const handleSend = () => {
    if (message.trim()) {
      onSend(message);
      setMessage("");
    }
  };

  const handleTranscription = (text: string) => {
    setTranscription(text);
  };

  const handleVoiceSend = (text: string) => {
    onSend(text);
    setTranscription(null);
  };

  // Show transcription preview if we have one
  if (transcription !== null) {
    return (
      <TranscriptionPreview
        text={transcription}
        onSend={handleVoiceSend}
        onCancel={() => setTranscription(null)}
        onReRecord={() => setTranscription(null)}
      />
    );
  }

  return (
    <div className="flex gap-2 p-4 border-t-4 border-black">
      {/* Text Input */}
      <input
        type="text"
        value={message}
        onChange={(e) => setMessage(e.target.value)}
        onKeyPress={(e) => e.key === "Enter" && handleSend()}
        placeholder={t("placeholder")}
        disabled={disabled}
        className="flex-1 p-3 border-4 border-black font-medium"
      />

      {/* Voice Input */}
      <VoiceInputButton
        onTranscription={handleTranscription}
        language={language}
        disabled={disabled}
      />

      {/* Send Button */}
      <button
        onClick={handleSend}
        disabled={disabled || !message.trim()}
        className={`
          p-3 border-4 border-black font-bold
          ${message.trim() ? "bg-black text-white" : "bg-gray-200"}
        `}
      >
        <Send size={24} />
      </button>
    </div>
  );
}
```

### Translation Keys
```json
// messages/en.json - Add voice section
{
  "chat": {
    "voice": {
      "holdToSpeak": "Hold to speak",
      "recording": "Recording...",
      "transcription": "Your message:",
      "send": "Send",
      "cancel": "Cancel",
      "reRecord": "Record again",
      "transcriptionFailed": "Could not transcribe audio",
      "microphoneError": "Could not access microphone"
    }
  }
}
```

### File Structure
```
lib/
└── elevenlabs.ts         # Add STT function

components/
└── chat/
    ├── VoiceInputButton.tsx
    ├── RecordingIndicator.tsx
    ├── TranscriptionPreview.tsx
    └── ChatInput.tsx       # Updated with voice
```

### Dependencies from Previous Stories
- Story 4.1: Chat interface
- Story 5.2: ElevenLabs client (extend)

---

## Testing

### Test Scenarios

1. **Voice Recording**
   - [ ] Hold button starts recording
   - [ ] Release button stops recording
   - [ ] Works on mobile (touch)

2. **Recording UI**
   - [ ] Indicator appears during recording
   - [ ] Duration counter updates
   - [ ] Button turns red

3. **Transcription**
   - [ ] Audio sent to ElevenLabs
   - [ ] Text returned correctly
   - [ ] Handles empty audio

4. **Preview & Edit**
   - [ ] Transcription shown for review
   - [ ] Text editable
   - [ ] Send/cancel/re-record work

5. **Language Support**
   - [ ] Works in English
   - [ ] Works in other languages
   - [ ] Language hint passed

6. **Permissions**
   - [ ] Microphone permission requested
   - [ ] Error shown if denied
   - [ ] Works in secure context

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-21 | 1.0 | Initial story draft | Bob (SM) |

---

## Dev Agent Record

### Implementation Summary

**Date**: 2025-12-23
**Status**: Completed

### Files Created

1. **`lib/elevenlabs.ts`** (modified)
   - Added `speechToText()` function for ElevenLabs STT API
   - Uses `scribe_v1` model for accurate transcription
   - Supports language hints for multilingual audio
   - Returns transcribed text and detected language

2. **`components/chat/VoiceInputButton.tsx`**
   - Push-to-talk button with hold-to-record interaction
   - Uses MediaRecorder API for audio capture
   - Handles browser microphone permissions
   - Shows loading spinner during transcription
   - Error handling with user-friendly messages

3. **`components/chat/RecordingIndicator.tsx`**
   - Floating indicator above voice button during recording
   - Animated waveform visualization
   - Live duration counter
   - Red background to indicate active recording

4. **`components/chat/TranscriptionPreview.tsx`**
   - Editable textarea for reviewing transcription
   - Send, Cancel, and Re-record buttons
   - Keyboard shortcuts (Enter to send, Escape to cancel)
   - Neobrutalist styling consistent with app theme

### Files Modified

1. **`components/chat/ChatWindow.tsx`**
   - Integrated voice input with CopilotKit
   - Uses `useCopilotChat` hook and `TextMessage` for sending
   - Voice input section below chat body
   - Transcription preview replaces voice button when active

2. **`messages/en.json`**
   - Added `chat.voice` section with all voice-related translations

### Technical Details

- Uses CopilotKit's `appendMessage` with `TextMessage` class
- MediaRecorder captures audio in webm format (with mp4 fallback)
- STT API called client-side using `NEXT_PUBLIC_ELEVENLABS_API_KEY`
- User's language preference passed as hint to improve accuracy

### Dependencies

- Story 5.2: ElevenLabs client (extended with STT)
- Story 4.1: Chat interface (extended with voice)

---

## QA Results
*To be filled by QA Agent*
