# Story 5.2: ElevenLabs TTS Integration

## Status

**Completed**

---

## Story

**As a** user,
**I want** to listen to my briefing in natural-sounding speech,
**so that** I can consume information while commuting or multitasking.

---

## Acceptance Criteria

1. ElevenLabs API integrated for text-to-speech conversion
2. Voice selected based on user's language preference
3. Audio file generated and stored (Convex file storage or URL)
4. Audio playable via browser audio element
5. Playback controls: play, pause, seek, speed adjustment
6. Audio generation happens in background (non-blocking)
7. Fallback to text display if audio generation fails

---

## Tasks / Subtasks

- [x] **Task 1: Create ElevenLabs Client** (AC: 1)
  - [x] Create `lib/elevenlabs.ts`
  - [x] Add `ELEVENLABS_API_KEY` to environment
  - [x] Implement TTS function
  - [x] Handle API errors

- [x] **Task 2: Configure Voice Selection** (AC: 2)
  - [x] Map languages to ElevenLabs voice IDs
  - [x] Select appropriate voice per language
  - [x] Default to multilingual voice (Bella)

- [x] **Task 3: Create TTS Action** (AC: 1, 3, 6)
  - [x] Create `convex/ai/tts.ts`
  - [x] Implement `generateAudio` action
  - [x] Store audio in Convex file storage
  - [x] Return storage ID and duration

- [x] **Task 4: Implement Background Generation** (AC: 6)
  - [x] Use Convex scheduled function
  - [x] Trigger on briefing creation
  - [x] Update briefing with audio status

- [x] **Task 5: Create Audio Player Hook** (AC: 4, 5)
  - [x] Create `hooks/useAudioPlayer.ts`
  - [x] Implement play, pause, seek
  - [x] Add speed adjustment (0.75x - 1.5x)
  - [x] Track playback progress

- [x] **Task 6: Implement Fallback** (AC: 7)
  - [x] Detect audio generation failure via audioStatus
  - [x] Display text transcript instead
  - [x] Show retry button via retryAudioGeneration action

- [x] **Task 7: Add Audio Status to Schema** (AC: 3)
  - [x] Add `audioStorageId` field to briefings
  - [x] Add `audioStatus` field (pending, ready, failed)
  - [x] Add `audioDuration` field

---

## Dev Notes

### ElevenLabs Client
[Source: architecture.md#external-services]

```typescript
// lib/elevenlabs.ts

interface ElevenLabsVoice {
  id: string;
  name: string;
  language: string;
}

// Voice IDs for supported languages
const VOICE_MAP: Record<string, string> = {
  en: "21m00Tcm4TlvDq8ikWAM", // Rachel (English)
  yo: "EXAVITQu4vr4xnSDxMaL", // Bella (multilingual)
  hi: "EXAVITQu4vr4xnSDxMaL", // Bella (multilingual)
  pt: "ErXwobaYiN019PkySvjV", // Antoni (Portuguese)
  tl: "EXAVITQu4vr4xnSDxMaL", // Bella (multilingual)
};

const ELEVENLABS_API_URL = "https://api.elevenlabs.io/v1";

export async function textToSpeech(
  text: string,
  language: string
): Promise<ArrayBuffer> {
  const apiKey = process.env.ELEVENLABS_API_KEY;
  if (!apiKey) {
    throw new Error("ELEVENLABS_API_KEY not configured");
  }

  const voiceId = VOICE_MAP[language] ?? VOICE_MAP.en;

  const response = await fetch(
    `${ELEVENLABS_API_URL}/text-to-speech/${voiceId}`,
    {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "xi-api-key": apiKey,
      },
      body: JSON.stringify({
        text,
        model_id: "eleven_multilingual_v2",
        voice_settings: {
          stability: 0.5,
          similarity_boost: 0.75,
          style: 0.5,
          use_speaker_boost: true,
        },
      }),
    }
  );

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`ElevenLabs API error: ${response.status} - ${error}`);
  }

  return response.arrayBuffer();
}

export function estimateAudioDuration(wordCount: number): number {
  // Average speaking rate: ~150 words per minute
  return Math.ceil((wordCount / 150) * 60);
}
```

### TTS Action
```typescript
// convex/ai/tts.ts
import { action, internalMutation } from "../_generated/server";
import { v } from "convex/values";
import { api, internal } from "../_generated/api";
import { textToSpeech, estimateAudioDuration } from "../../lib/elevenlabs";

export const generateAudio = action({
  args: {
    briefingId: v.id("briefings"),
  },
  handler: async (ctx, { briefingId }) => {
    const startTime = Date.now();

    // Get briefing
    const briefing = await ctx.runQuery(api.ai.briefings.getBriefingById, {
      id: briefingId,
    });

    if (!briefing) {
      throw new Error("Briefing not found");
    }

    try {
      // Update status to pending
      await ctx.runMutation(internal.ai.tts.updateAudioStatus, {
        briefingId,
        status: "pending",
      });

      // Generate audio
      const audioBuffer = await textToSpeech(briefing.script, briefing.language);

      // Store in Convex file storage
      const blob = new Blob([audioBuffer], { type: "audio/mpeg" });
      const storageId = await ctx.storage.store(blob);

      // Get the URL
      const audioUrl = await ctx.storage.getUrl(storageId);

      if (!audioUrl) {
        throw new Error("Failed to get storage URL");
      }

      // Estimate duration
      const duration = estimateAudioDuration(briefing.wordCount);

      // Update briefing with audio info
      await ctx.runMutation(internal.ai.tts.updateAudioStatus, {
        briefingId,
        status: "ready",
        audioUrl,
        audioDuration: duration,
      });

      console.log(`Audio generated in ${Date.now() - startTime}ms`);

      return { audioUrl, duration };
    } catch (error) {
      console.error("Audio generation failed:", error);

      await ctx.runMutation(internal.ai.tts.updateAudioStatus, {
        briefingId,
        status: "failed",
      });

      throw error;
    }
  },
});

export const updateAudioStatus = internalMutation({
  args: {
    briefingId: v.id("briefings"),
    status: v.union(v.literal("pending"), v.literal("ready"), v.literal("failed")),
    audioUrl: v.optional(v.string()),
    audioDuration: v.optional(v.number()),
  },
  handler: async (ctx, { briefingId, status, audioUrl, audioDuration }) => {
    await ctx.db.patch(briefingId, {
      audioStatus: status,
      ...(audioUrl && { audioUrl }),
      ...(audioDuration && { audioDuration }),
    });
  },
});
```

### Updated Briefings Schema
```typescript
// convex/schema.ts - Update briefings table
briefings: defineTable({
  userId: v.id("users"),
  corridorId: v.id("corridors"),
  type: v.union(
    v.literal("daily"),
    v.literal("weekly"),
    v.literal("progress")
  ),
  script: v.string(),
  wordCount: v.number(),
  language: v.string(),
  context: v.object({
    stage: v.string(),
    completedSteps: v.number(),
    totalSteps: v.number(),
    recentCompletions: v.array(v.string()),
  }),
  // Audio fields
  audioStatus: v.optional(
    v.union(v.literal("pending"), v.literal("ready"), v.literal("failed"))
  ),
  audioUrl: v.optional(v.string()),
  audioDuration: v.optional(v.number()), // seconds
  createdAt: v.number(),
})
  .index("by_user", ["userId"])
  .index("by_user_corridor", ["userId", "corridorId"])
  .index("by_user_type", ["userId", "type"]),
```

### Audio Player Hook
```typescript
// hooks/useAudioPlayer.ts
"use client";

import { useState, useRef, useEffect, useCallback } from "react";

interface UseAudioPlayerReturn {
  isPlaying: boolean;
  isPaused: boolean;
  duration: number;
  currentTime: number;
  progress: number;
  playbackRate: number;
  play: () => void;
  pause: () => void;
  toggle: () => void;
  seek: (time: number) => void;
  setPlaybackRate: (rate: number) => void;
  error: string | null;
}

export function useAudioPlayer(audioUrl: string | null): UseAudioPlayerReturn {
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const [isPlaying, setIsPlaying] = useState(false);
  const [isPaused, setIsPaused] = useState(false);
  const [duration, setDuration] = useState(0);
  const [currentTime, setCurrentTime] = useState(0);
  const [playbackRate, setPlaybackRateState] = useState(1);
  const [error, setError] = useState<string | null>(null);

  // Initialize audio element
  useEffect(() => {
    if (!audioUrl) return;

    const audio = new Audio(audioUrl);
    audioRef.current = audio;

    const handleLoadedMetadata = () => {
      setDuration(audio.duration);
    };

    const handleTimeUpdate = () => {
      setCurrentTime(audio.currentTime);
    };

    const handleEnded = () => {
      setIsPlaying(false);
      setIsPaused(false);
      setCurrentTime(0);
    };

    const handleError = () => {
      setError("Failed to load audio");
      setIsPlaying(false);
    };

    audio.addEventListener("loadedmetadata", handleLoadedMetadata);
    audio.addEventListener("timeupdate", handleTimeUpdate);
    audio.addEventListener("ended", handleEnded);
    audio.addEventListener("error", handleError);

    return () => {
      audio.pause();
      audio.removeEventListener("loadedmetadata", handleLoadedMetadata);
      audio.removeEventListener("timeupdate", handleTimeUpdate);
      audio.removeEventListener("ended", handleEnded);
      audio.removeEventListener("error", handleError);
    };
  }, [audioUrl]);

  const play = useCallback(() => {
    if (audioRef.current) {
      audioRef.current.play().catch((e) => setError(e.message));
      setIsPlaying(true);
      setIsPaused(false);
    }
  }, []);

  const pause = useCallback(() => {
    if (audioRef.current) {
      audioRef.current.pause();
      setIsPlaying(false);
      setIsPaused(true);
    }
  }, []);

  const toggle = useCallback(() => {
    if (isPlaying) {
      pause();
    } else {
      play();
    }
  }, [isPlaying, play, pause]);

  const seek = useCallback((time: number) => {
    if (audioRef.current) {
      audioRef.current.currentTime = time;
      setCurrentTime(time);
    }
  }, []);

  const setPlaybackRate = useCallback((rate: number) => {
    if (audioRef.current) {
      audioRef.current.playbackRate = rate;
      setPlaybackRateState(rate);
    }
  }, []);

  const progress = duration > 0 ? (currentTime / duration) * 100 : 0;

  return {
    isPlaying,
    isPaused,
    duration,
    currentTime,
    progress,
    playbackRate,
    play,
    pause,
    toggle,
    seek,
    setPlaybackRate,
    error,
  };
}
```

### Background Generation via Scheduler
```typescript
// convex/ai/briefings.ts - Add to generateBriefingScript

// After saving briefing, schedule audio generation
await ctx.scheduler.runAfter(0, api.ai.tts.generateAudio, {
  briefingId: savedBriefingId,
});
```

### File Structure
```
lib/
└── elevenlabs.ts         # ElevenLabs API client

hooks/
└── useAudioPlayer.ts     # Audio playback hook

convex/
└── ai/
    └── tts.ts            # Text-to-speech action
```

### Dependencies from Previous Stories
- Story 5.1: Briefing script generation

---

## Testing

### Test Scenarios

1. **Audio Generation**
   - [ ] TTS API called successfully
   - [ ] Audio file stored in Convex
   - [ ] URL accessible and playable

2. **Voice Selection**
   - [ ] English uses Rachel voice
   - [ ] Other languages use multilingual voice
   - [ ] Voice sounds natural

3. **Playback Controls**
   - [ ] Play starts audio
   - [ ] Pause stops audio
   - [ ] Seek works correctly
   - [ ] Speed adjustment works

4. **Background Generation**
   - [ ] Generation doesn't block UI
   - [ ] Status updates in real-time
   - [ ] Briefing playable when ready

5. **Error Handling**
   - [ ] Failed generation shows error
   - [ ] Text fallback displayed
   - [ ] Retry option available

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-21 | 1.0 | Initial story draft | Bob (SM) |

---

## Dev Agent Record

**Completed:** 2025-12-23

### Files Created
- `lib/elevenlabs.ts` - ElevenLabs TTS API client with voice mapping
- `convex/ai/tts.ts` - TTS generation action with Convex file storage
- `convex/ttsQueries.ts` - Internal queries and public queries for TTS
- `hooks/useAudioPlayer.ts` - Audio player hook with play/pause/seek/speed controls

### Files Modified
- `convex/schema.ts` - Added audioStatus, audioStorageId, audioDuration to briefings
- `convex/ai/briefings.ts` - Schedule audio generation after briefing creation
- `convex/briefingsQueries.ts` - Added briefingId to CachedBriefing interface

### Implementation Notes
- Uses ElevenLabs multilingual_v2 model for high-quality multilingual TTS
- Voice selection: Rachel for English, Antoni for Portuguese, Bella for other languages
- Audio stored in Convex file storage with storage ID reference
- Background generation via ctx.scheduler.runAfter(0, ...)
- Audio player hook supports 0.75x, 1x, 1.25x, 1.5x playback speeds
- Duration estimated at 150 words/minute
- Retry action available for failed audio generation

### Key Functions
- `textToSpeech` - ElevenLabs API client function
- `generateAudio` - Convex action to generate and store audio
- `retryAudioGeneration` - Retry failed audio generation
- `useAudioPlayer` - React hook for audio playback controls
- `getBriefingWithAudio` - Query to get briefing with audio URL

### Voice Configuration
| Language | Voice | ID |
|----------|-------|-----|
| en | Rachel | 21m00Tcm4TlvDq8ikWAM |
| pt | Antoni | ErXwobaYiN019PkySvjV |
| yo, hi, tl, ko, de, fr, es | Bella | EXAVITQu4vr4xnSDxMaL |

### Build Status
- TypeScript: ✅ Passed
- Convex deploy: ✅ Deployed

---

## QA Results
*To be filled by QA Agent*
